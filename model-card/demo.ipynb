{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdeKOEkv1Fe8"
   },
   "source": [
    "##### Copyright &copy; 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "c2jyGuiG1gHr"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23R0Z9RojXYW"
   },
   "source": [
    "# MLMD Model Card Toolkit Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfSQ-kX-MLEr"
   },
   "source": [
    "## Background\n",
    "\n",
    "This notebook demonstrates how to generate a model card usinge Model Card Toolkits with MLMD and TFX pipeline in a Jupyter/Colab environment. You can learn more about model cards at https://modelcards.withgoogle.com/about. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-ePgV0Lj68Q"
   },
   "source": [
    "### Import packages\n",
    "\n",
    "We import necessary packages, including standard TFX component classes and check the library versions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YIqpWK9efviJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bill\\software\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\data_types.py:191: UserWarning: RuntimeParameter is only supported on KubeflowDagRunner currently.\n",
      "  warnings.warn('RuntimeParameter is only supported on KubeflowDagRunner '\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tempfile\n",
    "import urllib\n",
    "\n",
    "import absl\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "tf.get_logger().propagate = False\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import tfx\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import Pusher\n",
    "from tfx.components import ResolverNode\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Transform\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model\n",
    "from tfx.types.standard_artifacts import ModelBlessing\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "import ml_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eZ4K18_DN2D8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.1\n",
      "TFX version: 0.22.1\n",
      "MLMD version: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "print('TFX version: {}'.format(tfx.__version__))\n",
    "print('MLMD version: {}'.format(ml_metadata.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufJKQ6OvkJlY"
   },
   "source": [
    "### Set up pipeline paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ad5JLpKbf6sN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bill\\software\\anaconda3\\lib\\site-packages\\tfx\n"
     ]
    }
   ],
   "source": [
    "# This is the root directory for your TFX pip package installation.\n",
    "_tfx_root = tfx.__path__[0]\n",
    "print(_tfx_root)\n",
    "\n",
    "# Set up logging.\n",
    "absl.logging.set_verbosity(absl.logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n2cMMAbSkGfX"
   },
   "source": [
    "### Download example data\n",
    "We download the example dataset for use in our TFX pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BywX6OUEhAqn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_data_root: C:\\Users\\bill\\AppData\\Local\\Temp\\tfx-dataejukzd_p\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/' \\\n",
    "   'adult.data'\n",
    "_data_root = tempfile.mkdtemp(prefix='tfx-data')\n",
    "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
    "urllib.request.urlretrieve(DATA_PATH, _data_filepath)\n",
    "print('_data_root: {}'.format(_data_root))\n",
    "\n",
    "columns = [\n",
    "  \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Marital-Status\",\n",
    "  \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital-Gain\", \"Capital-Loss\",\n",
    "  \"Hours-per-week\", \"Country\", \"Over-50K\"]\n",
    "\n",
    "with open(_data_filepath, 'r') as f:\n",
    "    content = f.read()\n",
    "    content = content.replace(\", <=50K\", ', 0').replace(\", >50K\", ', 1')\n",
    "\n",
    "with open(_data_filepath, 'w') as f:\n",
    "    f.write(','.join(columns) + '\\n' + content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "blZC1sIQOWfH"
   },
   "source": [
    "Take a quick look at the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c5YPeLPFOXaD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age,Workclass,fnlwgt,Education,Education-Num,Marital-Status,Occupation,Relationship,Race,Sex,Capital-Gain,Capital-Loss,Hours-per-week,Country,Over-50K\n",
      "39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, 0\n",
      "50, Self-emp-not-inc, 83311, Bachelors, 13, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 13, United-States, 0\n",
      "38, Private, 215646, HS-grad, 9, Divorced, Handlers-cleaners, Not-in-family, White, Male, 0, 0, 40, United-States, 0\n",
      "53, Private, 234721, 11th, 7, Married-civ-spouse, Handlers-cleaners, Husband, Black, Male, 0, 0, 40, United-States, 0\n",
      "28, Private, 338409, Bachelors, 13, Married-civ-spouse, Prof-specialty, Wife, Black, Female, 0, 0, 40, Cuba, 0\n",
      "37, Private, 284582, Masters, 14, Married-civ-spouse, Exec-managerial, Wife, White, Female, 0, 0, 40, United-States, 0\n",
      "49, Private, 160187, 9th, 5, Married-spouse-absent, Other-service, Not-in-family, Black, Female, 0, 0, 16, Jamaica, 0\n",
      "52, Self-emp-not-inc, 209642, HS-grad, 9, Married-civ-spouse, Exec-managerial, Husband, White, Male, 0, 0, 45, United-States, 1\n",
      "31, Private, 45781, Masters, 14, Never-married, Prof-specialty, Not-in-family, White, Female, 14084, 0, 50, United-States, 1\n"
     ]
    }
   ],
   "source": [
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ONIE_hdkPS4"
   },
   "source": [
    "### Create the InteractiveContext\n",
    "Last, we create an InteractiveContext, which will allow us to run TFX components interactively in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Rh6K5sUf9dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:InteractiveContext pipeline_root argument not provided: using temporary directory C:\\Users\\bill\\AppData\\Local\\Temp\\tfx-interactive-2020-08-03T11_46_22.725030-uipmuqq8 as root for pipeline outputs.\n",
      "WARNING:absl:InteractiveContext metadata_connection_config not provided: using SQLite ML Metadata database at C:\\Users\\bill\\AppData\\Local\\Temp\\tfx-interactive-2020-08-03T11_46_22.725030-uipmuqq8\\metadata.sqlite.\n"
     ]
    }
   ],
   "source": [
    "# Here, we create an InteractiveContext using default parameters. This will\n",
    "# use a temporary directory with an ephemeral ML Metadata database instance.\n",
    "# To use your own pipeline root or database, the optional properties\n",
    "# `pipeline_root` and `metadata_connection_config` may be passed to\n",
    "# InteractiveContext. Calls to InteractiveContext are no-ops outside of the\n",
    "# notebook.\n",
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdQWxfsVkzdJ"
   },
   "source": [
    "## Run TFX components interactively\n",
    "In the cells that follow, we create TFX components one-by-one, run each of them, and visualize their output artifacts. In this notebook, we won’t provide detailed explanations of each TFX component, but you can see what each does at [TFX Colab workshop](https://github.com/tensorflow/workshops/blob/master/tfx_labs/Lab_1_Pipeline_in_Colab.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9fwt9gQk3BR"
   },
   "source": [
    "### ExampleGen\n",
    "\n",
    "Create the `ExampleGen` component to split data into training and evaluation sets, convert the data into `tf.Example` format, and copy data into the `_tfx_root` directory for other components to access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyXjuMt8f-9u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Running driver for CsvExampleGen\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span = None\n",
      "INFO:absl:Running executor for CsvExampleGen\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Using 1 process(es) for Beam pipeline execution.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        var import_html = () => {\n",
       "          ['https://raw.githubusercontent.com/PAIR-code/facets/1.0.0/facets-dist/facets-jupyter.html'].forEach(href => {\n",
       "            var link = document.createElement('link');\n",
       "            link.rel = 'import'\n",
       "            link.href = href;\n",
       "            document.head.appendChild(link);\n",
       "          });\n",
       "        }\n",
       "        if ('import' in document.createElement('link')) {\n",
       "          import_html();\n",
       "        } else {\n",
       "          var webcomponentScript = document.createElement('script');\n",
       "          webcomponentScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js';\n",
       "          webcomponentScript.type = 'text/javascript';\n",
       "          webcomponentScript.onload = function(){\n",
       "            import_html();\n",
       "          };\n",
       "          document.head.appendChild(webcomponentScript);\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Processing input csv data C:\\Users\\bill\\AppData\\Local\\Temp\\tfx-dataejukzd_p\\* to TFExample.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Receiver() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2e0190c2dd16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mexample_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCsvExampleGen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexternal_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_data_root\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\experimental\\interactive\\interactive_context.py\u001b[0m in \u001b[0;36mrun_if_ipython\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m       \u001b[1;31m# __IPYTHON__ variable is set by IPython, see\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m       \u001b[1;31m# https://ipython.org/ipython-doc/rel-0.10.2/html/interactive/reference.html#embedding-ipython.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m       absl.logging.warning(\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\experimental\\interactive\\interactive_context.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, component, enable_cache, beam_pipeline_args)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mcomponent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_connection\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         beam_pipeline_args, additional_pipeline_args)\n\u001b[1;32m--> 168\u001b[1;33m     \u001b[0mexecution_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     return execution_result.ExecutionResult(\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\launcher\\base_component_launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    203\u001b[0m                          \u001b[0mexecution_decision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                          \u001b[0mexecution_decision\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                          execution_decision.exec_properties)\n\u001b[0m\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     absl.logging.info('Running publisher for %s',\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\tfx\\orchestration\\launcher\\in_process_component_launcher.py\u001b[0m in \u001b[0;36m_run_executor\u001b[1;34m(self, execution_id, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[0;32m     65\u001b[0m         executor_context)  # type: ignore\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexec_properties\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\tfx\\components\\example_gen\\base_example_gen_executor.py\u001b[0m in \u001b[0;36mDo\u001b[1;34m(self, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[0;32m    238\u001b[0m         (example_split\n\u001b[0;32m    239\u001b[0m          | 'WriteSplit[{}]'.format(split_name) >> _WriteSplit(\n\u001b[1;32m--> 240\u001b[1;33m              artifact_utils.get_split_uri(output_dict['examples'], split_name)))\n\u001b[0m\u001b[0;32m    241\u001b[0m       \u001b[1;31m# pylint: enable=expression-not-assigned, no-value-for-parameter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    553\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extra_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, test_runner_api)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m           \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m       \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_tempdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[1;34m(self, pipeline, options)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     self._latest_run_result = self.run_via_runner_api(\n\u001b[1;32m--> 173\u001b[1;33m         pipeline.to_runner_api(default_environment=self._default_environment))\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_latest_run_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\u001b[0m in \u001b[0;36mrun_via_runner_api\u001b[1;34m(self, pipeline_proto)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;31m# TODO(pabloem, BEAM-7514): Create a watermark manager (that has access to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;31m#   the teststream (if any), and all the stages).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_stages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstage_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\u001b[0m in \u001b[0;36mrun_stages\u001b[1;34m(self, stage_context, stages)\u001b[0m\n\u001b[0;32m    338\u001b[0m           stage_results = self._run_stage(\n\u001b[0;32m    339\u001b[0m               \u001b[0mrunner_execution_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m               \u001b[0mbundle_context_manager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m           )\n\u001b[0;32m    342\u001b[0m           monitoring_infos_by_stage[stage.name] = (\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\u001b[0m in \u001b[0;36m_run_stage\u001b[1;34m(self, runner_execution_context, bundle_context_manager)\u001b[0m\n\u001b[0;32m    517\u001b[0m               \u001b[0minput_timers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m               \u001b[0mexpected_timer_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m               bundle_manager)\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m       \u001b[0mfinal_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerge_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\u001b[0m in \u001b[0;36m_run_bundle\u001b[1;34m(self, runner_execution_context, bundle_context_manager, data_input, data_output, input_timers, expected_timer_output, bundle_manager)\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m     result, splits = bundle_manager.process_bundle(\n\u001b[1;32m--> 557\u001b[1;33m         data_input, data_output, input_timers, expected_timer_output)\n\u001b[0m\u001b[0;32m    558\u001b[0m     \u001b[1;31m# Now we collect all the deferred inputs remaining from bundle execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;31m# Deferred inputs can be:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[1;34m(self, inputs, expected_outputs, fired_timers, expected_output_timers, dry_run)\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mthread_pool_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared_unbounded_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m       for result, split_result in executor.map(execute, zip(part_inputs,  # pylint: disable=zip-builtin-not-iterating\n\u001b[1;32m--> 941\u001b[1;33m                                                             timer_inputs)):\n\u001b[0m\u001b[0;32m    942\u001b[0m         \u001b[0msplit_result_list\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msplit_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmerged_result\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    596\u001b[0m                     \u001b[1;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m                         \u001b[1;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    599\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\utils\\thread_pool_executor.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m       \u001b[1;31m# If the future wasn't cancelled, then attempt to execute it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_future\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Even though Python 2 futures library has #set_exection(),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(part_map_input_timers)\u001b[0m\n\u001b[0;32m    935\u001b[0m           \u001b[0minput_timers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m           \u001b[0mexpected_output_timers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m           dry_run)\n\u001b[0m\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mthread_pool_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshared_unbounded_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\fn_runner.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[1;34m(self, inputs, expected_outputs, fired_timers, expected_output_timers, dry_run)\u001b[0m\n\u001b[0;32m    835\u001b[0m             \u001b[0mprocess_bundle_descriptor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m             cache_tokens=[next(self._cache_token_generator)]))\n\u001b[1;32m--> 837\u001b[1;33m     \u001b[0mresult_future\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocess_bundle_req\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    839\u001b[0m     \u001b[0msplit_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\portability\\fn_api_runner\\worker_handlers.py\u001b[0m in \u001b[0;36mpush\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    350\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uid_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m       \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstruction_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'control_%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uid_counter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_instruction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mControlFuture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\worker\\sdk_worker.py\u001b[0m in \u001b[0;36mdo_instruction\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    478\u001b[0m       \u001b[1;31m# E.g. if register is set, this will call self.register(request.register))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m       return getattr(self, request_type)(\n\u001b[1;32m--> 480\u001b[1;33m           getattr(request, request_type), request.instruction_id)\n\u001b[0m\u001b[0;32m    481\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\worker\\sdk_worker.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[1;34m(self, request, instruction_id)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_profile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m           delayed_applications, requests_finalization = (\n\u001b[1;32m--> 515\u001b[1;33m               bundle_processor.process_bundle(instruction_id))\n\u001b[0m\u001b[0;32m    516\u001b[0m           \u001b[0mmonitoring_infos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbundle_processor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonitoring_infos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m           \u001b[0mmonitoring_infos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_cache_metrics_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\worker\\bundle_processor.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[1;34m(self, instruction_id)\u001b[0m\n\u001b[0;32m    976\u001b[0m           \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeam_fn_api_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mElements\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m             input_op_by_transform_id[element.transform_id].process_encoded(\n\u001b[1;32m--> 978\u001b[1;33m                 element.data)\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# Finish all operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\worker\\bundle_processor.py\u001b[0m in \u001b[0;36mprocess_encoded\u001b[1;34m(self, encoded_windowed_values)\u001b[0m\n\u001b[0;32m    216\u001b[0m       decoded_value = self.windowed_coder_impl.decode_from_stream(\n\u001b[0;32m    217\u001b[0m           input_stream, True)\n\u001b[1;32m--> 218\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoded_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmonitoring_infos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtag_to_pcollection_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\apache_beam\\runners\\worker\\operations.py\u001b[0m in \u001b[0;36moutput\u001b[1;34m(self, windowed_value, output_index)\u001b[0m\n\u001b[0;32m    330\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindowed_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;31m# type: (WindowedValue, int) -> None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m     \u001b[0mcython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReceiver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceivers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moutput_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreceive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindowed_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0madd_receiver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\software\\anaconda3\\lib\\site-packages\\Cython\\Shadow.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(type, *args, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__call__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Receiver() takes no arguments"
     ]
    }
   ],
   "source": [
    "example_gen = CsvExampleGen(input=external_input(_data_root))\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "880KkTAkPeUg"
   },
   "outputs": [],
   "source": [
    "artifact = example_gen.outputs['examples'].get()[0]\n",
    "print(artifact.split_names, artifact.uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J6vcbW_wPqvl"
   },
   "source": [
    "Let’s take a look at the first three training examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4XIXjiCPwzQ"
   },
   "outputs": [],
   "source": [
    "# Get the URI of the output artifact representing the training examples, which is a directory\n",
    "train_uri = os.path.join(example_gen.outputs['examples'].get()[0].uri, 'train')\n",
    "\n",
    "# Get the list of files in this directory (all compressed TFRecord files)\n",
    "tfrecord_filenames = [os.path.join(train_uri, name)\n",
    "                      for name in os.listdir(train_uri)]\n",
    "\n",
    "# Create a `TFRecordDataset` to read these files\n",
    "dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "\n",
    "# Iterate over the first 3 records and decode them.\n",
    "for tfrecord in dataset.take(3):\n",
    "  serialized_example = tfrecord.numpy()\n",
    "  example = tf.train.Example()\n",
    "  example.ParseFromString(serialized_example)\n",
    "  pp.pprint(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "csM6BFhtk5Aa"
   },
   "source": [
    "### StatisticsGen\n",
    "\n",
    "`StatisticsGen` takes as input the dataset we just ingested using `ExampleGen` and allows you to perform some analysis of your dataset using TensorFlow Data Validation (TFDV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MAscCCYWgA-9"
   },
   "outputs": [],
   "source": [
    "statistics_gen = StatisticsGen(\n",
    "    examples=example_gen.outputs['examples'])\n",
    "context.run(statistics_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0BDfOjGy048O"
   },
   "source": [
    "After `StatisticsGen` finishes running, we can visualize the outputted statistics. Try playing with the different plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tLjXy7K6Tp_G"
   },
   "outputs": [],
   "source": [
    "context.show(statistics_gen.outputs['statistics'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HLKLTO9Nk60p"
   },
   "source": [
    "### SchemaGen\n",
    "\n",
    "`SchemaGen` will take as input the statistics that we generated with `StatisticsGen`, looking at the training split by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygQvZ6hsiQ_J"
   },
   "outputs": [],
   "source": [
    "schema_gen = SchemaGen(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    infer_feature_shape=False)\n",
    "context.run(schema_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ec9vqDXpXeMb"
   },
   "outputs": [],
   "source": [
    "context.show(schema_gen.outputs['schema'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kZWWdbA-m7zp"
   },
   "source": [
    "To learn more about schemas, see [the SchemaGen documentation](https://www.tensorflow.org/tfx/guide/schemagen)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V1qcUuO9k9f8"
   },
   "source": [
    "### ExampleValidator\n",
    "`ExampleValidator` will take as input the statistics from `StatisticsGen`, and the schema from `SchemaGen`.\n",
    "\n",
    "By default, it compares the statistics from the evaluation split to the schema from the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XRlRUuGgiXks"
   },
   "outputs": [],
   "source": [
    "example_validator = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(example_validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDyAAozQcrk3"
   },
   "outputs": [],
   "source": [
    "context.show(example_validator.outputs['anomalies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPViEz5RlA36"
   },
   "source": [
    "### Transform\n",
    "\n",
    "`Transform` will take as input the data from `ExampleGen`, the schema from `SchemaGen`, as well as a module that contains user-defined Transform code.\n",
    "\n",
    "Let's see an example of user-defined Transform code below (for an introduction to the TensorFlow Transform APIs, [see the tutorial](https://www.tensorflow.org/tfx/tutorials/transform/simple)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuNSiUKb4YJf"
   },
   "outputs": [],
   "source": [
    "_census_income_constants_module_file = 'census_income_constants.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPjhXuIF4YJh"
   },
   "outputs": [],
   "source": [
    "%%writefile {_census_income_constants_module_file}\n",
    "\n",
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "MAX_CATEGORICAL_FEATURE_VALUES = [20]\n",
    "\n",
    "CATEGORICAL_FEATURE_KEYS = [\"Education-Num\"]\n",
    "\n",
    "\n",
    "DENSE_FLOAT_FEATURE_KEYS = [\"Capital-Gain\", \"Hours-per-week\", \"Capital-Loss\"]\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "BUCKET_FEATURE_KEYS = [\"Age\"]\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "VOCAB_SIZE = 200\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "OOV_SIZE = 10\n",
    "\n",
    "VOCAB_FEATURE_KEYS = [\"Workclass\", \"Education\", \"Marital-Status\", \"Occupation\", \n",
    "                      \"Relationship\", \"Race\", \"Sex\", \"Country\"]\n",
    "\n",
    "# Keys\n",
    "LABEL_KEY = \"Over-50K\"\n",
    "\n",
    "def transformed_name(key):\n",
    "  return key + '_xf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AJ9hBs94YJm"
   },
   "outputs": [],
   "source": [
    "_census_income_transform_module_file = 'census_income_transform.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYmxxx9A4YJn"
   },
   "outputs": [],
   "source": [
    "%%writefile {_census_income_transform_module_file}\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "import census_income_constants\n",
    "\n",
    "_DENSE_FLOAT_FEATURE_KEYS = census_income_constants.DENSE_FLOAT_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_KEYS = census_income_constants.VOCAB_FEATURE_KEYS\n",
    "_VOCAB_SIZE = census_income_constants.VOCAB_SIZE\n",
    "_OOV_SIZE = census_income_constants.OOV_SIZE\n",
    "_FEATURE_BUCKET_COUNT = census_income_constants.FEATURE_BUCKET_COUNT\n",
    "_BUCKET_FEATURE_KEYS = census_income_constants.BUCKET_FEATURE_KEYS\n",
    "_CATEGORICAL_FEATURE_KEYS = census_income_constants.CATEGORICAL_FEATURE_KEYS\n",
    "_LABEL_KEY = census_income_constants.LABEL_KEY\n",
    "_transformed_name = census_income_constants.transformed_name\n",
    "\n",
    "\n",
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "  Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "  Returns:\n",
    "    Map from string feature key to transformed feature operations.\n",
    "  \"\"\"\n",
    "  outputs = {}\n",
    "  for key in _DENSE_FLOAT_FEATURE_KEYS:\n",
    "    # Preserve this feature as a dense float, setting nan's to the mean.\n",
    "    outputs[_transformed_name(key)] = tft.scale_to_z_score(\n",
    "        _fill_in_missing(inputs[key]))\n",
    "\n",
    "  for key in _VOCAB_FEATURE_KEYS:\n",
    "    # Build a vocabulary for this feature.\n",
    "    outputs[_transformed_name(key)] = tft.compute_and_apply_vocabulary(\n",
    "        _fill_in_missing(inputs[key]),\n",
    "        top_k=_VOCAB_SIZE,\n",
    "        num_oov_buckets=_OOV_SIZE)\n",
    "\n",
    "  for key in _BUCKET_FEATURE_KEYS:\n",
    "    outputs[_transformed_name(key)] = tft.bucketize(\n",
    "        _fill_in_missing(inputs[key]), _FEATURE_BUCKET_COUNT)\n",
    "\n",
    "  for key in _CATEGORICAL_FEATURE_KEYS:\n",
    "    outputs[_transformed_name(key)] = _fill_in_missing(inputs[key])\n",
    "\n",
    "  label = _fill_in_missing(inputs[_LABEL_KEY])\n",
    "  outputs[_transformed_name(_LABEL_KEY)] = label\n",
    "  \n",
    "  return outputs\n",
    "\n",
    "\n",
    "def _fill_in_missing(x):\n",
    "  \"\"\"Replace missing values in a SparseTensor.\n",
    "  Fills in missing values of `x` with '' or 0, and converts to a dense tensor.\n",
    "  Args:\n",
    "    x: A `SparseTensor` of rank 2.  Its dense shape should have size at most 1\n",
    "      in the second dimension.\n",
    "  Returns:\n",
    "    A rank 1 tensor where missing values of `x` have been filled in.\n",
    "  \"\"\"\n",
    "  default_value = '' if x.dtype == tf.string else 0\n",
    "  return tf.squeeze(\n",
    "      tf.sparse.to_dense(\n",
    "          tf.SparseTensor(x.indices, x.values, [x.dense_shape[0], 1]),\n",
    "          default_value),\n",
    "      axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jHfhth_GiZI9"
   },
   "outputs": [],
   "source": [
    "transform = Transform(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=os.path.abspath(_census_income_transform_module_file))\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SClrAaEGR1O5"
   },
   "outputs": [],
   "source": [
    "transform.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBJFtnl6lCg9"
   },
   "source": [
    "### Trainer\n",
    "Let's see an example of user-defined model code below (for an introduction to the TensorFlow Keras APIs, [see the tutorial](https://www.tensorflow.org/guide/keras)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N1376oq04YJt"
   },
   "outputs": [],
   "source": [
    "_census_income_trainer_module_file = 'census_income_trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nf9UuNng4YJu"
   },
   "outputs": [],
   "source": [
    "%%writefile {_census_income_trainer_module_file}\n",
    "\n",
    "from typing import List, Text\n",
    "\n",
    "import os\n",
    "import absl\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "from tfx.components.trainer.executor import TrainerFnArgs\n",
    "\n",
    "import census_income_constants\n",
    "\n",
    "_DENSE_FLOAT_FEATURE_KEYS = census_income_constants.DENSE_FLOAT_FEATURE_KEYS\n",
    "_VOCAB_FEATURE_KEYS = census_income_constants.VOCAB_FEATURE_KEYS\n",
    "_VOCAB_SIZE = census_income_constants.VOCAB_SIZE\n",
    "_OOV_SIZE = census_income_constants.OOV_SIZE\n",
    "_FEATURE_BUCKET_COUNT = census_income_constants.FEATURE_BUCKET_COUNT\n",
    "_BUCKET_FEATURE_KEYS = census_income_constants.BUCKET_FEATURE_KEYS\n",
    "_CATEGORICAL_FEATURE_KEYS = census_income_constants.CATEGORICAL_FEATURE_KEYS\n",
    "_MAX_CATEGORICAL_FEATURE_VALUES = census_income_constants.MAX_CATEGORICAL_FEATURE_VALUES\n",
    "_LABEL_KEY = census_income_constants.LABEL_KEY\n",
    "_transformed_name = census_income_constants.transformed_name\n",
    "\n",
    "\n",
    "def _transformed_names(keys):\n",
    "  return [_transformed_name(key) for key in keys]\n",
    "\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "  \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "  return tf.data.TFRecordDataset(\n",
    "      filenames,\n",
    "      compression_type='GZIP')\n",
    "\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "  \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  @tf.function\n",
    "  def serve_tf_examples_fn(serialized_tf_examples):\n",
    "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "    feature_spec = tf_transform_output.raw_feature_spec()\n",
    "    feature_spec.pop(_LABEL_KEY)\n",
    "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "\n",
    "    transformed_features = model.tft_layer(parsed_features)\n",
    "    transformed_features.pop(_transformed_name(_LABEL_KEY))\n",
    "\n",
    "    return model(transformed_features)\n",
    "\n",
    "  return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  transformed_feature_spec = (\n",
    "      tf_transform_output.transformed_feature_spec().copy())\n",
    "\n",
    "  dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "      file_pattern=file_pattern,\n",
    "      batch_size=batch_size,\n",
    "      features=transformed_feature_spec,\n",
    "      reader=_gzip_reader_fn,\n",
    "      label_key=_transformed_name(_LABEL_KEY))\n",
    "\n",
    "  return dataset\n",
    "\n",
    "\n",
    "def _build_keras_model(hidden_units: List[int] = None) -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model.\n",
    "\n",
    "  Args:\n",
    "    hidden_units: [int], the layer sizes of the DNN (input layer first).\n",
    "\n",
    "  Returns:\n",
    "    A keras Model.\n",
    "  \"\"\"\n",
    "  real_valued_columns = [\n",
    "      tf.feature_column.numeric_column(key, shape=())\n",
    "      for key in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key, num_buckets=_VOCAB_SIZE + _OOV_SIZE, default_value=0)\n",
    "      for key in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns += [\n",
    "      tf.feature_column.categorical_column_with_identity(\n",
    "          key, num_buckets=_FEATURE_BUCKET_COUNT, default_value=0)\n",
    "      for key in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
    "  ]\n",
    "  categorical_columns += [\n",
    "      tf.feature_column.categorical_column_with_identity(  # pylint: disable=g-complex-comprehension\n",
    "          key,\n",
    "          num_buckets=num_buckets,\n",
    "          default_value=0) for key, num_buckets in zip(\n",
    "              _transformed_names(_CATEGORICAL_FEATURE_KEYS),\n",
    "              _MAX_CATEGORICAL_FEATURE_VALUES)\n",
    "  ]\n",
    "  indicator_column = [\n",
    "      tf.feature_column.indicator_column(categorical_column)\n",
    "      for categorical_column in categorical_columns\n",
    "  ]\n",
    "\n",
    "  model = _wide_and_deep_classifier(\n",
    "      # TODO(b/139668410) replace with premade wide_and_deep keras model\n",
    "      wide_columns=indicator_column,\n",
    "      deep_columns=real_valued_columns,\n",
    "      dnn_hidden_units=hidden_units or [100, 70, 50, 25])\n",
    "  return model\n",
    "\n",
    "\n",
    "def _wide_and_deep_classifier(wide_columns, deep_columns, dnn_hidden_units):\n",
    "  \"\"\"Build a simple keras wide and deep model.\n",
    "\n",
    "  Args:\n",
    "    wide_columns: Feature columns wrapped in indicator_column for wide (linear)\n",
    "      part of the model.\n",
    "    deep_columns: Feature columns for deep part of the model.\n",
    "    dnn_hidden_units: [int], the layer sizes of the hidden DNN.\n",
    "\n",
    "  Returns:\n",
    "    A Wide and Deep Keras model\n",
    "  \"\"\"\n",
    "  # Following values are hard coded for simplicity in this example,\n",
    "  # However prefarably they should be passsed in as hparams.\n",
    "\n",
    "  # Keras needs the feature definitions at compile time.\n",
    "  # TODO(b/139081439): Automate generation of input layers from FeatureColumn.\n",
    "  input_layers = {\n",
    "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype=tf.float32)\n",
    "      for colname in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
    "  }\n",
    "  input_layers.update({\n",
    "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
    "      for colname in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
    "  })\n",
    "  input_layers.update({\n",
    "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
    "      for colname in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
    "  })\n",
    "  input_layers.update({\n",
    "      colname: tf.keras.layers.Input(name=colname, shape=(), dtype='int32')\n",
    "      for colname in _transformed_names(_CATEGORICAL_FEATURE_KEYS)\n",
    "  })\n",
    "\n",
    "  # TODO(b/161816639): SparseFeatures for feature columns + Keras.\n",
    "  deep = tf.keras.layers.DenseFeatures(deep_columns)(input_layers)\n",
    "  for numnodes in dnn_hidden_units:\n",
    "    deep = tf.keras.layers.Dense(numnodes)(deep)\n",
    "  wide = tf.keras.layers.DenseFeatures(wide_columns)(input_layers)\n",
    "\n",
    "  output = tf.keras.layers.Dense(\n",
    "      1, activation='sigmoid')(\n",
    "          tf.keras.layers.concatenate([deep, wide]))\n",
    "\n",
    "  model = tf.keras.Model(input_layers, output)\n",
    "  model.compile(\n",
    "      loss='binary_crossentropy',\n",
    "      optimizer=tf.keras.optimizers.Adam(lr=0.001),\n",
    "      metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "  model.summary(print_fn=absl.logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: TrainerFnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "  # Number of nodes in the first layer of the DNN\n",
    "  first_dnn_layer_size = 100\n",
    "  num_dnn_layers = 4\n",
    "  dnn_decay_factor = 0.7\n",
    "\n",
    "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "  train_dataset = _input_fn(fn_args.train_files, tf_transform_output, 40)\n",
    "  eval_dataset = _input_fn(fn_args.eval_files, tf_transform_output, 40)\n",
    "\n",
    "  model = _build_keras_model(\n",
    "      # Construct layers sizes with exponetial decay\n",
    "      hidden_units=[\n",
    "          max(2, int(first_dnn_layer_size * dnn_decay_factor**i))\n",
    "          for i in range(num_dnn_layers)\n",
    "      ])\n",
    "\n",
    "  # This log path might change in the future.\n",
    "  log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
    "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir, update_freq='batch')\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps,\n",
    "      callbacks=[tensorboard_callback])\n",
    "\n",
    "  signatures = {\n",
    "      'serving_default':\n",
    "          _get_serve_tf_examples_fn(model,\n",
    "                                    tf_transform_output).get_concrete_function(\n",
    "                                        tf.TensorSpec(\n",
    "                                            shape=[None],\n",
    "                                            dtype=tf.string,\n",
    "                                            name='examples')),\n",
    "  }\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "429-vvCWibO0"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    module_file=os.path.abspath(_census_income_trainer_module_file),\n",
    "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
    "    examples=transform.outputs['transformed_examples'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=50))\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FmPftrv0lEQy"
   },
   "source": [
    "### Evaluator\n",
    "The `Evaluator` component computes model performance metrics over the evaluation set. It uses the [TensorFlow Model Analysis](https://www.tensorflow.org/tfx/model_analysis/get_started) library. \n",
    "\n",
    "`Evaluator` will take as input the data from `ExampleGen`, the trained model from `Trainer`, and slicing configuration. The slicing configuration allows you to slice your metrics on feature values. See an example of this configuration below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVhfzzh9PDEx"
   },
   "outputs": [],
   "source": [
    "from google.protobuf.wrappers_pb2 import BoolValue\n",
    "\n",
    "eval_config = tfma.EvalConfig(\n",
    "    model_specs=[\n",
    "        # This assumes a serving model with signature 'serving_default'. If\n",
    "        # using estimator based EvalSavedModel, add signature_name: 'eval' and \n",
    "        # remove the label_key.\n",
    "        tfma.ModelSpec(label_key=\"Over-50K\")\n",
    "    ],\n",
    "    metrics_specs=[\n",
    "        tfma.MetricsSpec(\n",
    "            # The metrics added here are in addition to those saved with the\n",
    "            # model (assuming either a keras model or EvalSavedModel is used).\n",
    "            # Any metrics added into the saved model (for example using\n",
    "            # model.compile(..., metrics=[...]), etc) will be computed\n",
    "            # automatically.\n",
    "            # To add validation thresholds for metrics saved with the model,\n",
    "            # add them keyed by metric name to the thresholds map.\n",
    "            metrics=[\n",
    "                tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                tfma.MetricConfig(class_name='FairnessIndicators',\n",
    "                                  config='{ \"thresholds\": [0.5] }'),\n",
    "            ]\n",
    "        )\n",
    "    ],\n",
    "    slicing_specs=[\n",
    "        # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
    "        tfma.SlicingSpec(),\n",
    "        # Data can be sliced along a feature column. In this case, data is\n",
    "        # sliced by feature column Race and Sex.\n",
    "        tfma.SlicingSpec(feature_keys=['Race']),\n",
    "        tfma.SlicingSpec(feature_keys=['Sex']),\n",
    "        tfma.SlicingSpec(feature_keys=['Race', 'Sex']),\n",
    "    ],\n",
    "    options = tfma.Options(compute_confidence_intervals=BoolValue(value=True))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zjcx8g6mihSt"
   },
   "outputs": [],
   "source": [
    "# Use TFMA to compute a evaluation statistics over features of a model and\n",
    "# validate them against a baseline.\n",
    "evaluator = Evaluator(\n",
    "    examples=example_gen.outputs['examples'],\n",
    "    model=trainer.outputs['model'],\n",
    "    eval_config=eval_config)\n",
    "context.run(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4GghePOTJxL"
   },
   "outputs": [],
   "source": [
    "evaluator.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5TMskWe9LL0"
   },
   "source": [
    "Using the `evaluation` output we can show the default visualization of global metrics on the entire evaluation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U729j5X5QQUQ"
   },
   "outputs": [],
   "source": [
    "context.show(evaluator.outputs['evaluation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sYM7Tnrf7Ffr"
   },
   "source": [
    "## Populate Properties from ModelCard with Model Card Toolkit\n",
    "Now that we’ve set up our TFX pipeline, we will use the Model Card Toolkit to extract key artifacts from the run and populate a Model Card.\n",
    "\n",
    "\n",
    "### Connect to the MLMD store used by the InteractiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDgnuUtV7IaN"
   },
   "outputs": [],
   "source": [
    "from ml_metadata.metadata_store import metadata_store\n",
    "from IPython import display\n",
    "\n",
    "mlmd_store = metadata_store.MetadataStore(context.metadata_connection_config)\n",
    "model_uri = trainer.outputs[\"model\"].get()[0].uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHIy8C7a7KZh"
   },
   "source": [
    "### Use Model Card Toolkit\n",
    "\n",
    "#### Initialize the Model Card Toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lw5Xcn4xnNQB"
   },
   "outputs": [],
   "source": [
    "from model_card_toolkit import ModelCardToolkit\n",
    "\n",
    "mct = ModelCardToolkit(mlmd_store=mlmd_store, model_uri=model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYSWqygOOBVl"
   },
   "source": [
    "#### Create Model Card workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUFl_cNj8fPN"
   },
   "outputs": [],
   "source": [
    "model_card = mct.scaffold_assets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrnPOUcAOStf"
   },
   "source": [
    "#### Annotate more infomation into Model Card.\n",
    "\n",
    "It is also important to document model information that might be important to downstream users, such as its limitations, intended use cases, trade offs, and ethical considerations. For each of these sections, we can directly add new JSON objects to represent this information. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvFUltDAB3O5"
   },
   "outputs": [],
   "source": [
    "model_card.model_details.name = 'Census Income Classifier'\n",
    "model_card.model_details.overview = (\n",
    "    'This is a wide and deep Keras model which aims to classify whether or not '\n",
    "    'an individual has an income of over $50,000 based on various demographic '\n",
    "    'features. The model is trained on the UCI Census Income Dataset. This is '\n",
    "    'not a production model, and this dataset has traditionally only been used '\n",
    "    'for research purposes. In this Model Card, you can review quantitative '\n",
    "    'components of the model’s performance and data, as well as information '\n",
    "    'about the model’s intended uses, limitations, and ethical considerations.'\n",
    ")\n",
    "model_card.model_details.owners = [\n",
    "  {'name': 'Model Cards Team', 'contact': 'model-cards@google.com'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-b12rEyq7QXG"
   },
   "outputs": [],
   "source": [
    "model_card.considerations.use_cases = [\n",
    "    'This dataset that this model was trained on was originally created to '\n",
    "    'support the machine learning community in conducting empirical analysis '\n",
    "    'of ML algorithms. The Adult Data Set can be used in fairness-related '\n",
    "    'studies that compare inequalities across sex and race, based on '\n",
    "    'people’s annual incomes.'\n",
    "]\n",
    "model_card.considerations.limitations = [\n",
    "    'This is a class-imbalanced dataset across a variety of sensitive classes.'\n",
    "    ' The ratio of male-to-female examples is about 2:1 and there are far more'\n",
    "    ' examples with the “white” attribute than every other race combined. '\n",
    "    'Furthermore, the ratio of $50,000 or less earners to $50,000 or more '\n",
    "    'earners is just over 3:1. Due to the imbalance across income levels, we '\n",
    "    'can see that our true negative rate seems quite high, while our true '\n",
    "    'positive rate seems quite low. This is true to an even greater degree '\n",
    "    'when we only look at the “female” sub-group, because there are even '\n",
    "    'fewer female examples in the $50,000+ earner group, causing our model to '\n",
    "    'overfit these examples. To avoid this, we can try various remediation '\n",
    "    'strategies in future iterations (e.g. undersampling, hyperparameter '\n",
    "    'tuning, etc), but we may not be able to fix all of the fairness issues.'\n",
    "]\n",
    "model_card.considerations.ethical_considerations = [{\n",
    "    'name':\n",
    "        'We risk expressing the viewpoint that the attributes in this dataset '\n",
    "        'are the only ones that are predictive of someone’s income, even '\n",
    "        'though we know this is not the case.',\n",
    "    'mitigation_strategy':\n",
    "        'As mentioned, some interventions may need to be performed to address '\n",
    "        'the class imbalances in the dataset.'\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zo9xHyAcVl6h"
   },
   "source": [
    "#### Filter and Add Graphs.\n",
    "\n",
    "We can filter the graphs generated by the TFX components to include those most relevant for the Model Card using the function defined below. In this example, we filter for `race` and `sex`, two potentially sensitive attributes. \n",
    "\n",
    "Each Model Card will have up to three sections for graphs -- training dataset statistics, evaluation dataset statistics, and quantitative analysis of our model’s performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ju8A6CfP0z1O"
   },
   "outputs": [],
   "source": [
    "# These are the graphs that will appear in the Quantiative Analysis portion of \n",
    "# the Model Card. Feel free to add or remove from this list. \n",
    "TARGET_EVAL_GRAPH_NAMES = [\n",
    "  'fairness_indicators_metrics/false_positive_rate@0.5',\n",
    "  'fairness_indicators_metrics/false_negative_rate@0.5',\n",
    "  'binary_accuracy',\n",
    "  'example_count | Race_X_Sex',\n",
    "]\n",
    "\n",
    "# These are the graphs that will appear in both the Train Set and Eval Set \n",
    "# portions of the Model Card. Feel free to add or remove from this list. \n",
    "TARGET_DATASET_GRAPH_NAMES = [\n",
    "  'counts | Race',\n",
    "  'counts | Sex',\n",
    "]\n",
    "\n",
    "def filter_graphs(graphics, target_graph_names):\n",
    "  result = []\n",
    "  for graph in graphics:\n",
    "    for target_graph_name in target_graph_names:\n",
    "      if graph.name.startswith(target_graph_name):\n",
    "        result.append(graph)\n",
    "  result.sort(key=lambda g: g.name)\n",
    "  return result\n",
    "\n",
    "# Populating the three different sections using the filter defined above. To \n",
    "# see all the graphs available in a section, we can iterate through each of the\n",
    "# different collections. \n",
    "model_card.quantitative_analysis.graphics.collection = filter_graphs(\n",
    "    model_card.quantitative_analysis.graphics.collection, TARGET_EVAL_GRAPH_NAMES)\n",
    "model_card.model_parameters.data.eval.graphics.collection = filter_graphs(\n",
    "    model_card.model_parameters.data.eval.graphics.collection, TARGET_DATASET_GRAPH_NAMES)\n",
    "model_card.model_parameters.data.train.graphics.collection = filter_graphs(\n",
    "    model_card.model_parameters.data.train.graphics.collection, TARGET_DATASET_GRAPH_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nT83xLb7ZH1J"
   },
   "source": [
    "We then add (optional) descriptions for each of the each of the graph sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssvy6H0m4L6H"
   },
   "outputs": [],
   "source": [
    "model_card.model_parameters.data.train.graphics.description = (\n",
    "    'This section includes graphs displaying the class distribution for the '\n",
    "    '“Race” and “Sex” attributes in our training dataset. We chose to '\n",
    "    'show these graphs in particular because we felt it was important that '\n",
    "    'users see the class imbalance.'\n",
    ")\n",
    "model_card.model_parameters.data.eval.graphics.description = (\n",
    "    'Like the training set, we provide graphs showing the class distribution '\n",
    "    'of the data we used to evaluate our model’s performance. '\n",
    ")\n",
    "model_card.quantitative_analysis.graphics.description = (\n",
    "    'These graphs show how the model performs for data sliced by “Race”, '\n",
    "    '“Sex” and the intersection of these attributes. The metrics we chose '\n",
    "    'to display are “Accuracy”, “False Positive Rate”, and “False '\n",
    "    'Negative Rate”, because we anticipated that the class imbalances might '\n",
    "    'cause our model to underperform for certain groups.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7V0pJLB8jqJ"
   },
   "outputs": [],
   "source": [
    "mct.update_model_card_json(model_card)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SOYofSZKOMZx"
   },
   "source": [
    "#### Generate the Model Card.\n",
    "We can now display the Model Card in HTML format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lTlYCH-5aA-H"
   },
   "outputs": [],
   "source": [
    "html = mct.export_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sd68Ih928vr9"
   },
   "outputs": [],
   "source": [
    "display.display(display.HTML(html))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MLMD Model Card Toolkit Demo",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
