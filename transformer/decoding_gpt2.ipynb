{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'C:/Users/bill/Documents/projects/data/chatbot'\n",
    "train_file = f'{PATH}/gpt/train.txt'\n",
    "test_file = f'{PATH}/gpt/test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "Given input of size $x\\in \\mathbb{R}^{B \\times T \\times H}$, layer norm is defined by\n",
    "\n",
    "$$\n",
    "    \\mathbf{y}_t = \\frac{\\mathbf{x}_t - \\mu_t}{\\sqrt{\\sigma_t + \\epsilon}}\\odot \\boldsymbol{\\gamma}_t + \\boldsymbol{\\beta}_t\n",
    "$$\n",
    "where \n",
    "- $\\gamma \\in \\mathbb{R}^{T\\times H}$ is the weight term\n",
    "- $\\beta \\in \\mathbb{R}^{T\\times H}$ is the bias term\n",
    "- $\\mu_t$ is the average over all the hidden units\n",
    "- $\\sigma_t$ is the standard deviation over all the hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        # \n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # take the mean and standard deviation over the last dimension\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        v = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(v + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(8, 10, 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([10, 768]) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], grad_fn=<SliceBackward>)\n",
      "bias torch.Size([10, 768]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 768])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LayerNorm(input.size()[1:])\n",
    "output = m(input)\n",
    "for k, v in m.named_parameters():\n",
    "    print(k, v.shape, v[0][0:10])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([10, 768]) tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], grad_fn=<SliceBackward>)\n",
      "bias torch.Size([10, 768]) tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 10, 768])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.LayerNorm(input.size()[1:])\n",
    "output = m(input)\n",
    "for k, v in m.named_parameters():\n",
    "    print(k, v.shape, v[0][0:10])\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $x\\in \\mathbb{R}^{B \\times T \\times H}$, `Conv1D(F, H)` is just a linear layer that outputs $y\\in \\mathbb{R}^{B \\times T \\times F}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1D(nn.Module):\n",
    "    \"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n",
    "            Basically works like a Linear layer but the weights are transposed\n",
    "        \"\"\"\n",
    "    def __init__(self, F, H):\n",
    "        super(Conv1D, self).__init__()\n",
    "        self.F = F\n",
    "        w = torch.empty(H, F)\n",
    "        nn.init.normal_(w, std=0.02)\n",
    "        self.weight = nn.Parameter(w) # H x F\n",
    "        self.bias = nn.Parameter(torch.zeros(F)) # F\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x in B x T x H\n",
    "        # x.size()[:-1] => B x T\n",
    "        # size_out = B x T x F\n",
    "        size_out = x.size()[:-1] + (self.F,)\n",
    "        # Wx + b => (B x T) x F\n",
    "        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n",
    "        # convert back to B x T x F\n",
    "        x = x.view(*size_out)\n",
    "        return x\n",
    "    \n",
    "class Conv1DNew(nn.Module):\n",
    "    def __init__(self, F, H):\n",
    "        super(Conv1DNew, self).__init__()\n",
    "        self.l = nn.Linear(H, F)\n",
    "        nn.init.normal_(self.l.weight, std=0.02)\n",
    "        self.l.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([8, 10, 768]), output: torch.Size([8, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "m = Conv1D(20, 768)\n",
    "output = m(input)\n",
    "\n",
    "n = Conv1DNew(20, 768)\n",
    "n.l.weight = nn.Parameter(m.weight.t())\n",
    "output1 = n(input)\n",
    "\n",
    "assert torch.all(torch.eq(output, output1))\n",
    "print(f'input: {input.shape}, output: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, nx, n_ctx, config, scale=False):\n",
    "        super(Attention, self).__init__()\n",
    "        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n",
    "        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n",
    "        assert n_state % config.n_head == 0\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n",
    "        self.n_head = config.n_head\n",
    "        self.split_size = n_state\n",
    "        self.scale = scale\n",
    "        self.c_attn = Conv1D(n_state * 3, nx)\n",
    "        self.c_proj = Conv1D(n_state, nx)\n",
    "\n",
    "    def _attn(self, q, k, v):\n",
    "        w = torch.matmul(q, k)\n",
    "        if self.scale:\n",
    "            w = w / math.sqrt(v.size(-1))\n",
    "        nd, ns = w.size(-2), w.size(-1)\n",
    "        b = self.bias[:, :, ns-nd:ns, :ns]\n",
    "        w = w * b - 1e10 * (1 - b)\n",
    "        w = nn.Softmax(dim=-1)(w)\n",
    "        return torch.matmul(w, v)\n",
    "\n",
    "    def merge_heads(self, x):\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n",
    "        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n",
    "\n",
    "    def split_heads(self, x, k=False):\n",
    "        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n",
    "        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n",
    "        if k:\n",
    "            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n",
    "        else:\n",
    "            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        x = self.c_attn(x)\n",
    "        query, key, value = x.split(self.split_size, dim=2)\n",
    "        query = self.split_heads(query)\n",
    "        key = self.split_heads(key, k=True)\n",
    "        value = self.split_heads(value)\n",
    "        if layer_past is not None:\n",
    "            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n",
    "            key = torch.cat((past_key, key), dim=-1)\n",
    "            value = torch.cat((past_value, value), dim=-2)\n",
    "        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n",
    "        a = self._attn(query, key, value)\n",
    "        a = self.merge_heads(a)\n",
    "        a = self.c_proj(a)\n",
    "        return a, present"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
