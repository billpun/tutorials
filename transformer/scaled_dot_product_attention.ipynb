{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Basics\n",
    "In a typical encoder-decoder framework, we want to summarize the information from $\\mathbf{x} = x_1,\\dots,x_I$ by some representions/hidden states. These hidden states will then be used in the decoder to predict $\\mathbf{y} = y_1,\\dots,y_J$. \n",
    "\n",
    "The encoding operation can be best described by $h_{i+1}=f^E(h_i, x_i)\\in \\mathbb{R}^K$, where $f^E$ is the chosen encoding function, and $h_{i+1}$ is the new hidden state vector encoded with additional information from $x_i$. At the end, $h_I$ should have encoded all the necessary information from the given input sequence $\\mathbf{x}$ for the decoder to consume. \n",
    "\n",
    "The decoder is then initialized with $s_0=h_I$ and produces both the target $y_j$ and the corresponding hidden state $s_j$ for a given $y_{j-1}$ and $s_{j-1}$. The problem with this approach is that $h_I$ is often limited in providing sufficient information for the decoder to accuractely compute $y_1,\\dots,y_J$ when $m$ is large. An obvious approach is to reintroduce $h_1,\\dots,h_{I-1}$ to see if they can provide useful information for the decoder.\n",
    "\n",
    "Toward this end, a neural network is used to learn the importance of $h_1,\\dots,h_{I}$ and see how much each of them contribute to the prediction of $y_j$. More precisely, the contribution/attention weight for a given $i$ and $j$ is\n",
    "\\\\[\n",
    "    a_{j,i} = \\text{softmax}_{i=1,\\dots,I}\\left( \\text{score}(h_i, s_{j-1})\\right)\n",
    "\\\\]\n",
    "where the scoring function (many other options are available) is \n",
    "\\\\[\n",
    "    \\text{score}(s_{j-1}, h_i) = \\frac{h_is_{j-1}^\\intercal}{\\sqrt{K}}.\n",
    "\\\\]\n",
    "The scoring function measures how close $h_i$ and $s_{j-1}$ are, and the scaling is done to prevent extremely large values resulting from the dot product when $K$ is large (so that the resulting score is hidden size independent). As the final step, we multiply the attention weight with the encoder hidden states to produce a vector, context vector, with information that the decoder can use. More precisely, for $y_j$, the context vector $c_j\\in \\mathbb{R}^K$ is\n",
    "\\\\[\n",
    "    c_j = a_{j,:}^\\intercal h_:\n",
    "\\\\]\n",
    "The context vector is then added to decode the hidden state vector for $j$:\n",
    "\\\\[\n",
    "    s_{j} = f( s_{j-1}, y_{j-1}, c_j).\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled-dot-product Attention \n",
    "\n",
    "To put the attention in the transformer context, we have the context matrix of the scaled-dot-product attention \n",
    "\n",
    "\\\\[\n",
    "    c = \\text{softmax}\\left(\\frac{q k^\\intercal }{\\sqrt{H}}\\right)v\n",
    "\\\\]\n",
    "where\n",
    "- $s$: $q\\in R^{J\\times K}$ is the query,\n",
    "- $h$: $k\\in R^{I\\times K}$ is the key,\n",
    "- $h$: $v\\in R^{I\\times K}$ is the value,\n",
    "- $c\\in R^{I\\times K}$ is the attention, and\n",
    "- the softmax function is applied for each row.\n",
    "\n",
    "The score $qk^\\intercal$ is normalized by $K$ to prevent the attention score to be overly large before applying the softmax. The operation is summarized on the left hand side of Figure 2 below.\n",
    "\n",
    "<img src=\"images/scaled_mhead_attentions.png\" alt=\"Figure 2. From ‘Attention Is All You Need’ by Vaswani et al.\" style=\"width:80%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B: batch size (could be mostly ignored)\n",
    "# I, J: seq, time, max # of words in a sentence\n",
    "# K: hidden layer, size of word embedding\n",
    "B, I, J, K = 3, 9, 10, 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dropout and softmax layers\n",
    "#dropout = nn.Dropout(0.1)\n",
    "softmax = nn.Softmax(dim=-1) # softmax for each j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the query, key, and value\n",
    "q = torch.rand((B, J, K))\n",
    "k = torch.rand((B, I, K))\n",
    "v = torch.rand((B, I, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each query has a key\n",
    "# (B, J, K) x (B, K, I) -> (B, J, I)\n",
    "s = torch.matmul(q, k.transpose(-2, -1).contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale to prevent value from being too large because of the large bmm\n",
    "s = s / math.sqrt(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "#s = torch.exp(s)\n",
    "#s = s / s.sum(-1, keepdim=True)\n",
    "# or\n",
    "a = F.softmax(s, dim=-1)\n",
    "#s = dropout(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each keyed-weight of a query multiplies v to get the final attention score\n",
    "# (B, J, I) x (B, I, K) -> (B, J, K)\n",
    "c = torch.bmm(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weights: torch.Size([10, 9])\n",
      "context matrix: torch.Size([10, 18])\n",
      "first batch, first j, sum(i) = 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "print('attention weights: {}'.format(a.shape[1:]))\n",
    "print('context matrix: {}'.format(c.shape[1:]))\n",
    "\n",
    "# ensure that the weight summed up to 1\n",
    "print('first batch, first j, sum(i) = {}'.format(sum(a[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "    a = F.softmax(torch.matmul(q, k.transpose(-2, -1).contiguous()) / math.sqrt(k.shape[-1]), dim=-1)\n",
    "    return torch.matmul(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the function is correct\n",
    "torch.all(c.eq(scaled_dot_product_attention(q, k, v)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
