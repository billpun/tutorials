{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding\n",
    "\n",
    "Using a vector to represent the position of a token in a sentence in order to counter the fact that there is no recurrence and no convolution to capture positional information of the tokens. The encoding function is\n",
    "\n",
    "\\\\[\n",
    "    p_{i,k} = \\begin{cases}\n",
    "        \\sin(w_{i, k}) & \\text{if $k$ is even} \\\\\n",
    "        \\cos(w_{i, k}) & \\text{if $k$ is odd}\n",
    "    \\end{cases}\n",
    "\\\\]\n",
    "where\n",
    "\\\\[\n",
    "    w_{i, k} = \\frac{i}{10000^{2 k / K}}.\n",
    "\\\\]\n",
    "\n",
    "This function ensures that there is a unique position vector for each time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, I, J, K, H = 3, 9, 10, 18, 3\n",
    "\n",
    "pe = torch.zeros(I, K)\n",
    "for i in range(I):\n",
    "    for k in range(0, K, 2):\n",
    "        w = i / pow(10000, (2 * k) / K)\n",
    "        pe[i, k] = math.sin(w)\n",
    "        pe[i, k + 1] = math.cos(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, I, J, K, H = 1, 9, 10, 10, 3\n",
    "\n",
    "pe = torch.zeros(I, K)\n",
    "for i in range(I):\n",
    "    for k in range(0, K, 2):\n",
    "        w = i / pow(10000, (2 * k) / K)\n",
    "        pe[i, k] = math.sin(w)\n",
    "        pe[i, k + 1] = math.cos(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, K, I):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        pe = torch.zeros(I, K)\n",
    "        for i in range(I):\n",
    "            for k in range(0, K, 2):\n",
    "                w = i / pow(10000, (2 * k) / K)\n",
    "                pe[i, k] = math.sin(w)\n",
    "                pe[i, k + 1] = math.cos(w)\n",
    "                \n",
    "        pe = pe.unsqueeze(0)\n",
    "        # to make sure that it is not considered as a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x * math.sqrt(self.K)\n",
    "        K = x.size(1)\n",
    "        # note that the positional encoding is added to the input vector instead of concat\n",
    "        x = x + Variable(self.pe[:,:K], requires_grad=False).cuda()\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
