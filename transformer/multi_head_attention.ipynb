{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mult-head Attention\n",
    "\n",
    "The idea is to split the given query, key, and value into $H$ many heads, and run attention for each of them separately.\n",
    "\n",
    "<img src=\"images/scaled_mhead_attentions.png\" alt=\"Figure 2. From ‘Attention Is All You Need’ by Vaswani et al.\" style=\"width:80%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(k.shape[-1])\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1) # add batch\n",
    "        scores = scores.masked_fill(mask==0, -1e9)\n",
    "    a = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        a = dropout(a)\n",
    "    return torch.matmul(a, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, I, J, K, H = 3, 9, 10, 18, 3\n",
    "\n",
    "# determine the dimension of q, k, v after splitting into H many segments\n",
    "d_k = K // H\n",
    "q = torch.rand((B, J, K))\n",
    "k = torch.rand((B, I, K))\n",
    "v = torch.rand((B, I, K))\n",
    "\n",
    "# for each head\n",
    "cs = []\n",
    "for h in range(H):\n",
    "    # grab the corresponding segment\n",
    "    qh = q[:,:,(h * d_k):((h + 1) * d_k)]\n",
    "    kh = k[:,:,(h * d_k):((h + 1) * d_k)]\n",
    "    vh = v[:,:,(h * d_k):((h + 1) * d_k)]\n",
    "    # get context from attention\n",
    "    cs.append(scaled_dot_product_attention(qh, kh, vh))\n",
    "# concat them all\n",
    "c1 = torch.cat(cs, dim=-1).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix way of doing this\n",
    "# B x J/I x K -> B x J/I x H x d_k -> B x H x J/I x d_k\n",
    "qh = q.view(B, -1, H, d_k).transpose(1, 2)\n",
    "kh = k.view(B, -1, H, d_k).transpose(1, 2)\n",
    "vh = v.view(B, -1, H, d_k).transpose(1, 2)\n",
    "c2 = scaled_dot_product_attention(qh, kh, vh)\\\n",
    "    .transpose(1, 2).contiguous().view(B, -1, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(c1.eq(c2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_attention(q, k, v, H):\n",
    "    B, _, K = q.shape\n",
    "    d_k = K // H\n",
    "    \n",
    "    # B x J/I x K -> B x J/I x H x d_k -> B x H x J/I x d_k\n",
    "    qh = q.view(B, -1, H, d_k).transpose(1, 2)\n",
    "    kh = k.view(B, -1, H, d_k).transpose(1, 2)\n",
    "    vh = v.view(B, -1, H, d_k).transpose(1, 2)\n",
    "\n",
    "    c = scaled_dot_product_attention(qh, kh, vh)\\\n",
    "        .transpose(1, 2).contiguous().view(B, -1, K)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(c1.eq(multi_head_attention(q, k, v, H)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, H, K, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.K = K\n",
    "        self.d_k = K // H\n",
    "        self.H = H\n",
    "        \n",
    "        self.q_linear = nn.Linear(K, K)\n",
    "        self.v_linear = nn.Linear(K, K)\n",
    "        self.k_linear = nn.Linear(K, K)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.out = nn.Linear(K, K)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \n",
    "        B = q.size(0)\n",
    "        \n",
    "        # perform linear operation and split into h heads\n",
    "        \n",
    "        qh = self.q_linear(q).view(B, -1, self.H, self.d_k).transpose(1,2)\n",
    "        kh = self.k_linear(k).view(B, -1, self.H, self.d_k).transpose(1,2)\n",
    "        vh = self.v_linear(v).view(B, -1, self.H, self.d_k).transpose(1,2)\n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = scaled_dot_product_attention(\n",
    "            qh, kh, vh, mask, self.dropout)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1,2).contiguous().view(B, -1, self.K)\n",
    "        output = self.out(concat)\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
